---
layout: article
title:  "sqoop常用导数据命令集合"
categories: hadoop
toc: true
ads: true
#image:

#teaser: /teaser/defalut.jpg
#
---
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#orgheadline1">1. 列出mysql数据库中的所有数据库</a></li>
<li><a href="#orgheadline2">2. 连接mysql并列出数据库中的表</a></li>
<li><a href="#orgheadline3">3. 将关系型数据的表结构复制到hive中</a></li>
<li><a href="#orgheadline4">4. 将数据从关系数据库导入文件到hive表中</a></li>
<li><a href="#orgheadline5">5. 将hive中的表数据导入到mysql数据库表中</a></li>
<li><a href="#orgheadline6">6. 将数据从关系数据库导入文件到hive表中，&#x2013;query语句使用</a></li>
<li><a href="#orgheadline7">7. 将数据从关系数据库导入文件到hive表中，&#x2013;columns &#x2013;where语句使用</a></li>
<li><a href="#orgheadline8">8. sqoop help import : common arguments</a></li>
</ul>
</div>
</div>

# 列出mysql数据库中的所有数据库<a id="orgheadline1"></a>

sqoop list-databases &#x2013;connect jdbc:mysql://localhost:3306/ &#x2013;username dyh &#x2013;password 000000

# 连接mysql并列出数据库中的表<a id="orgheadline2"></a>

sqoop list-tables &#x2013;connect jdbc:mysql://localhost:3306/ &#x2013;username dyh &#x2013;password 000000

# 将关系型数据的表结构复制到hive中<a id="orgheadline3"></a>

sqoop create-hive-table &#x2013;connect jdbc:mysql://localhost:3306/test &#x2013;table users &#x2013;username dyh

&#x2013;password 000000 &#x2013;hive-table users  &#x2013;fields-terminated-by "\\0001"  &#x2013;lines-terminated-by "\n";

- 参数说明：

&#x2013;fields-terminated-by "\\0001"  是设置每列之间的分隔符，"\\0001"是ASCII码中的1，它也是hive的默认行内分隔符， 而sqoop的默认行内分隔符为"，"

&#x2013;lines-terminated-by "\n"  设置的是每行之间的分隔符，此处为换行符，也是默认的分隔符；

注意：只是复制表的结构，表中的内容没有复制

# 将数据从关系数据库导入文件到hive表中<a id="orgheadline4"></a>

sqoop import &#x2013;connect jdbc:mysql://localhost:3306/test &#x2013;username dyh &#x2013;password 000000

&#x2013;table users &#x2013;hive-import &#x2013;hive-table users -m 2 &#x2013;fields-terminated-by "\\0001";

- 参数说明：
    -m 2 表示由两个map作业执行
    &#x2013;fields-terminated-by "\\0001" 需同创建hive表时保持一致。
- 导入格式：
    在默认的情况下，sqoop会将我们导入的数据保存为逗号分隔的文件。如果导入数据的字段内容中存在分隔符，我们可以制定分隔符、字段保卫字符和转移字符。使用命令行参数可以制定分隔符、文件格式、压缩以及对导入过程进行更细粒度的控制。
- 导入控制：
    Sqoop 可以指定导入表得部分列。用户也可以在查询中加入where自字句，一次来限定需要导入的记录。用户提供得where字句会在任务分解之前执行。

# 将hive中的表数据导入到mysql数据库表中<a id="orgheadline5"></a>

sqoop export &#x2013;connect jdbc:mysql://192.168.20.3306/test &#x2013;username dyh &#x2013;password 000000
&#x2013;table users &#x2013;export-dir /usr/hive/warehouse/users/part-m-00000
&#x2013;input-fields-terminated-by '\\0001'

- 注意：
    1.  在进行导入之前，mysql中的表userst必须已经提前创建好了。

    2.  jdbc:mysql://192.168.118:3306/test中的IP地址改成localhost会报异常

# 将数据从关系数据库导入文件到hive表中，&#x2013;query语句使用<a id="orgheadline6"></a>

sqoop import &#x2013;append &#x2013;connect jdbc:mysql://192.168.20.118:3306/test &#x2013;username dyh &#x2013;password 000000 &#x2013;query "select id,age,name from userinfos where \\$CONDITIONS"  -m 1  &#x2013;target-dir /user/hive/warehouse/userinfos2 &#x2013;fields-terminated-by ",";

# 将数据从关系数据库导入文件到hive表中，&#x2013;columns &#x2013;where语句使用<a id="orgheadline7"></a>

sqoop import &#x2013;append &#x2013;connect jdbc:mysql://192.168.20.118:3306/test &#x2013;username dyh &#x2013;password 000000 &#x2013;table userinfos &#x2013;columns "id,age,name"  &#x2013;where "id > 3 and (age = 88 or age = 80)"  -m 1  &#x2013;target-dir /user/hive/warehouse/userinfos2 &#x2013;fields-terminated-by ",";

注意：&#x2013;target-dir /user/hive/warehouse/userinfos2   可以用  &#x2013;hive-import &#x2013;hive-table userinfos2 进行替换

# 数据导入和一致性 <a id="orgheadline8"></a>
在向HDFS导入数据时，重要的是要确保访问的数据源的一致性快照。
从一个数据库中并行读取数据的Map任务分别运行在不同得进程中，因此他们不肯呢过共享同一个数据库食物。保证一致性的最好方法就是在导入数据
的时候不允许任何对表中现有数据进行更新的过程。
# 直接模式导入 <a id="orgheadline8"></a>
例如MySql的mysqldump能够以大于JDBC的吞吐率从表中读取数据。在Sqoop的文档中将这种使用外部工具的方法成为“直接模式”

# 导入的数据与HIVE
如果想直接从数据库将数据导入到Hive，可以将三个步骤（将数据导入HDFS；创建HIVE表；将HDFS中的数据导入HIVE）缩短为一个步骤。
在进行导入时，Sqoop可以生成Hive表得定义，然后直接将数据导入Hive表。如果我们还没有执行过导入操作，就可以使用下面得命令，
根据MySQL中的数据直接穿件Hive中的widgests表。
   % sqoop import --connect jdbc:mysql://localhost/hadoopguide > --table widgets -m 1 --hive-import

# sqoop help import : common arguments<a id="orgheadline8"></a>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<tbody>
<tr>
<td class="org-left">argu</td>
<td class="org-left">description</td>
</tr>


<tr>
<td class="org-left">&#x2013;connect <jdbc-uri></td>
<td class="org-left">Specify JDBC connect string</td>
</tr>


<tr>
<td class="org-left">&#x2013;connection-manager <class-name></td>
<td class="org-left">Specify connection manager class to use</td>
</tr>


<tr>
<td class="org-left">&#x2013;driver <class-name></td>
<td class="org-left">Manually specify JDBC driver class to use</td>
</tr>


<tr>
<td class="org-left">&#x2013;hadoop-home <dir></td>
<td class="org-left">Overrite $HADOOP<sub>HOME</sub></td>
</tr>


<tr>
<td class="org-left">&#x2013;help</td>
<td class="org-left">Print usage instructions</td>
</tr>


<tr>
<td class="org-left">-P</td>
<td class="org-left">Read password from console</td>
</tr>


<tr>
<td class="org-left">&#x2013;password <password></td>
<td class="org-left">Set authentication password</td>
</tr>


<tr>
<td class="org-left">&#x2013;username <username></td>
<td class="org-left">Set authentication username</td>
</tr>


<tr>
<td class="org-left">&#x2013;verbose</td>
<td class="org-left">Print more information while working</td>
</tr>


<tr>
<td class="org-left">&#x2013;connection-param-file <filename></td>
<td class="org-left">Optional properties file that provide connection parameters</td>
</tr>


<tr>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
</tr>
</tbody>
</table>
